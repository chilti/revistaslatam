# Gu√≠a para Completar la Base de Datos OpenAlex

## Situaci√≥n Actual

Ya ejecutaste `load2.py` que carg√≥:
- ‚úÖ `works` (3,404,980 registros)
- ‚úÖ `works_authorships` (11,201,204 registros)

Pero faltan tablas cr√≠ticas:
- ‚ùå `sources` (0 registros)
- ‚ùå `institutions` (0 registros)
- ‚ùå `works_primary_location` (no existe)
- ‚ùå `works_open_access` (no existe)

## Soluci√≥n: Script `load_missing_tables.py`

He creado un script complementario que carga las tablas faltantes usando la misma estrategia que tu `load2.py`.

---

## Pasos para Completar la Base de Datos

### **Paso 1: Verificar el Snapshot**

Aseg√∫rate de tener estas carpetas en tu snapshot:

```
openalex-snapshot/data/
‚îú‚îÄ‚îÄ sources/           ‚Üê Necesario
‚îú‚îÄ‚îÄ institutions/      ‚Üê Necesario
‚îî‚îÄ‚îÄ works/            ‚Üê Ya lo usaste con load2.py
```

### **Paso 2: Ajustar Configuraci√≥n**

Edita `load_missing_tables.py` si es necesario:

```python
DB_PARAMS = {
    "host": "localhost",
    "port": 5432,
    "database": "openalex",  # ‚Üê Cambiar si usas otro nombre
    "user": "postgres",
    "password": "tu_contasena" 
}

SNAPSHOT_DIR = "./openalex-snapshot/data"  # ‚Üê Ajustar ruta si es necesario
```

### **Paso 3: Ejecutar el Script**

```bash
python load_missing_tables.py
```

**Proceso**:
1. Carga `sources` (revistas LATAM)
2. Carga `institutions` (instituciones LATAM)
3. Carga `works_primary_location` (relaci√≥n work‚Üísource)
4. Carga `works_open_access` (informaci√≥n de OA)

**Tiempo estimado**: 30-60 minutos (dependiendo del tama√±o del snapshot)

---

## Qu√© Hace Cada Tabla

### **1. `sources` (Revistas)**

**Datos**:
- ID de la revista
- ISSN
- Nombre
- Editorial
- Pa√≠s
- Si est√° en DOAJ
- N√∫mero de trabajos

**Importancia**: **CR√çTICA** - Permite identificar qu√© revistas son latinoamericanas.

---

### **2. `institutions` (Instituciones)**

**Datos**:
- ID de la instituci√≥n
- ROR
- Nombre
- **Pa√≠s** ‚Üê Cr√≠tico
- Tipo
- N√∫mero de trabajos

**Importancia**: **CR√çTICA** - Permite identificar el pa√≠s de cada trabajo por las afiliaciones de los autores.

---

### **3. `works_primary_location`**

**Datos**:
- work_id
- **source_id** ‚Üê Cr√≠tico (relaciona work con revista)
- is_oa
- landing_page_url
- pdf_url
- license
- version

**Importancia**: **CR√çTICA** - Es el puente entre `works` y `sources`.

---

### **4. `works_open_access`**

**Datos**:
- work_id
- is_oa
- oa_status (gold, green, hybrid, bronze, closed)
- oa_url
- any_repository_has_fulltext

**Importancia**: **ALTA** - Necesario para calcular m√©tricas de acceso abierto.

---

## Despu√©s de Cargar las Tablas

### **Paso 1: Verificar**

```bash
python diagnose_postgres.py
```

**Deber√≠as ver**:
```
sources                       :          X,XXX  ‚Üê Ya no 0
institutions                  :         XX,XXX  ‚Üê Ya no 0
works_primary_location        :      X,XXX,XXX  ‚Üê Ya no "Table not found"
works_open_access             :      X,XXX,XXX  ‚Üê Ya no "Table not found"
```

### **Paso 2: Crear √çndices**

```bash
psql -U postgres -d openalex -f create_indexes.sql
```

Esto acelerar√° las consultas significativamente.

### **Paso 3: Extraer Datos**

Ahora puedes usar el script completo:

```bash
python data_collector_postgres.py
```

Este script:
- ‚úÖ Identifica revistas LATAM con precisi√≥n (usando `sources.country_code`)
- ‚úÖ Extrae works de esas revistas (usando `works_primary_location`)
- ‚úÖ Incluye informaci√≥n de OA (usando `works_open_access`)
- ‚úÖ Determina pa√≠s por instituciones (usando `institutions.country_code`)

---

## Comparaci√≥n: Antes vs Despu√©s

### **Antes** (solo works + authorships)

```
Estrategia: Detecci√≥n de texto en afiliaciones
Precisi√≥n: ~50-80%
Velocidad: Lenta (scan completo)
Script: data_collector_postgres_simple.py
```

### **Despu√©s** (con todas las tablas)

```
Estrategia: Joins con institutions.country_code
Precisi√≥n: ~95%+
Velocidad: R√°pida (con √≠ndices)
Script: data_collector_postgres.py
```

---

## Soluci√≥n de Problemas

### **Error: "No se encuentra la carpeta sources"**

**Problema**: El snapshot no tiene la carpeta `sources`.

**Soluci√≥n**: 
1. Verifica que descargaste el snapshot completo
2. Ajusta `SNAPSHOT_DIR` en el script

### **Error: "relation already exists"**

**Problema**: La tabla ya existe (probablemente vac√≠a).

**Soluci√≥n**: Elimina la tabla vac√≠a:
```sql
DROP TABLE IF EXISTS openalex.sources;
DROP TABLE IF EXISTS openalex.institutions;
DROP TABLE IF EXISTS openalex.works_primary_location;
DROP TABLE IF EXISTS openalex.works_open_access;
```

Luego vuelve a ejecutar el script.

### **Error de memoria**

**Problema**: El script consume mucha memoria.

**Soluci√≥n**: El script ya usa buffers y commits incrementales, pero si a√∫n hay problemas:
1. Procesa menos archivos a la vez
2. Aumenta la memoria de PostgreSQL en `postgresql.conf`:
   ```
   shared_buffers = 2GB
   work_mem = 256MB
   ```

---

## Estimaci√≥n de Espacio en Disco

### **Tablas a Cargar**

| Tabla | Registros Estimados | Espacio Estimado |
|-------|---------------------|------------------|
| `sources` | ~8,000 | ~5 MB |
| `institutions` | ~50,000 | ~30 MB |
| `works_primary_location` | ~3,400,000 | ~200 MB |
| `works_open_access` | ~3,400,000 | ~150 MB |
| **Total** | | **~385 MB** |

M√°s √≠ndices: ~200 MB adicionales

**Total necesario**: ~600 MB

---

## Verificaci√≥n Final

Despu√©s de completar todo, ejecuta:

```bash
# 1. Verificar base de datos
python diagnose_postgres.py

# 2. Extraer datos
python data_collector_postgres.py

# 3. Verificar archivos generados
python diagnose_data.py

# 4. Calcular m√©tricas
python precompute_metrics_parallel_optimized.py

# 5. Ver dashboard
streamlit run dashboard.py
```

---

## Resumen

1. ‚úÖ Ya tienes: `works`, `works_authorships` (cargados con `load2.py`)
2. üîß Ejecuta: `python load_missing_tables.py`
3. ‚úÖ Obtendr√°s: `sources`, `institutions`, `works_primary_location`, `works_open_access`
4. üöÄ Usa: `python data_collector_postgres.py` (script completo, alta precisi√≥n)

¬°Esto te dar√° un sistema completo y preciso para analizar las revistas latinoamericanas!
