# Data Collector Simplificado - PostgreSQL

## Situaci√≥n

Tu base de datos PostgreSQL tiene un snapshot **parcial** de OpenAlex con solo estas tablas:

‚úÖ **Disponibles**:
- `works` (3,404,980 registros)
- `works_authorships` (11,201,204 registros)

‚ùå **Faltantes/Vac√≠as**:
- `sources` (0 registros)
- `institutions` (0 registros)
- `works_primary_location` (no existe)
- `works_concepts` (0 registros)
- `works_open_access` (no existe)

## Estrategia Alternativa

Como no tenemos la tabla `institutions` con `country_code`, el script `data_collector_postgres_simple.py` usa una **estrategia heur√≠stica**:

### üîç **C√≥mo Identifica Trabajos Latinoamericanos**

1. **Lee `raw_affiliation_string`** de `works_authorships`
2. **Busca nombres de pa√≠ses** en el texto (ej: "Mexico", "Brasil", "Universidad de Chile")
3. **Extrae el pa√≠s** usando un diccionario de mapeo
4. **Filtra** solo trabajos con al menos una afiliaci√≥n LATAM
5. **Determina pa√≠s principal** por conteo de afiliaciones

### üìù **Ejemplo**

```
Affiliation: "Universidad Nacional Aut√≥noma de M√©xico, Mexico City"
           ‚Üì
Detecta: "mexico" ‚Üí C√≥digo: MX
           ‚Üì
Incluye el trabajo como mexicano
```

## Uso

### **Modo Completo** (procesa todos los 3.4M de trabajos)

```bash
python data_collector_postgres_simple.py
```

**Advertencia**: Esto puede tomar **varias horas** porque:
- Procesa 3,404,980 trabajos en lotes de 10,000
- Para cada lote, extrae y analiza las afiliaciones
- Filtra por texto (no por √≠ndices)

### **Modo de Prueba** (solo primeros 100k trabajos)

```bash
python data_collector_postgres_simple.py --test
```

√ötil para verificar que funciona antes de procesar todo.

## Salida

### **Archivo Generado**

`data/latin_american_works.parquet`

### **Columnas**

- `id`: OpenAlex ID del trabajo
- `doi`: DOI
- `title`: T√≠tulo
- `display_name`: Nombre para mostrar
- `publication_year`: A√±o de publicaci√≥n
- `publication_date`: Fecha de publicaci√≥n
- `type`: Tipo de trabajo
- `cited_by_count`: N√∫mero de citas
- `is_retracted`: ¬øEst√° retractado?
- `is_paratext`: ¬øEs paratexto?
- `cited_by_api_url`: URL de la API de citas
- `abstract_inverted_index`: √çndice invertido del abstract
- `language`: Idioma
- `authorships`: JSON con autores y afiliaciones
- `country_code`: **Pa√≠s principal detectado**
- `latam_countries`: **Lista de todos los pa√≠ses LATAM detectados**
- `download_date`: Fecha de descarga

## Limitaciones

### ‚ö†Ô∏è **Precisi√≥n Reducida**

Como usamos detecci√≥n de texto en lugar de la tabla `institutions`:

- ‚úÖ **Detecta** afiliaciones que mencionan el pa√≠s expl√≠citamente
- ‚ùå **Pierde** afiliaciones que solo mencionan la instituci√≥n sin el pa√≠s
- ‚ùå **Puede fallar** con abreviaturas o nombres en otros idiomas

**Ejemplo de lo que se pierde**:
```
"UNAM" ‚Üí No detecta que es M√©xico (a menos que diga "UNAM, Mexico")
"USP" ‚Üí No detecta que es Brasil
```

### üìä **Estimaci√≥n de Cobertura**

- **Mejor caso**: ~70-80% de trabajos LATAM reales
- **Peor caso**: ~50-60% de trabajos LATAM reales

Depende de qu√© tan completas sean las afiliaciones en el snapshot.

## Optimizaci√≥n

### **Crear √çndice en Afiliaciones**

Para acelerar la b√∫squeda:

```sql
-- √çndice en raw_affiliation_string para b√∫squedas de texto
CREATE INDEX idx_wa_affiliation 
ON openalex.works_authorships 
USING gin(to_tsvector('english', raw_affiliation_string));

-- √çndice en work_id
CREATE INDEX idx_wa_work_id 
ON openalex.works_authorships(work_id);
```

Ejecutar:
```bash
psql -U postgres -d openalex -c "CREATE INDEX idx_wa_work_id ON openalex.works_authorships(work_id);"
```

## Progreso

El script muestra progreso cada lote:

```
Processing batch: 0 - 10,000
  ‚Üí Found 234 LATAM works in this batch
  ‚Üí Total LATAM works so far: 234

Processing batch: 10,000 - 20,000
  ‚Üí Found 189 LATAM works in this batch
  ‚Üí Total LATAM works so far: 423

üíæ Saving intermediate results...
  ‚úì Saved 423 works
```

Guarda autom√°ticamente cada 50,000 trabajos procesados.

## Interrupci√≥n y Reanudaci√≥n

### **Interrumpir**

Presiona `Ctrl+C` cuando veas "üíæ Saving intermediate results..."

### **Reanudar**

El script **NO** tiene modo resume autom√°tico. Para evitar re-procesar:

1. **Opci√≥n A**: Renombra el archivo existente
   ```bash
   mv data/latin_american_works.parquet data/latin_american_works_backup.parquet
   ```

2. **Opci√≥n B**: Modifica el script para empezar en un offset espec√≠fico
   ```python
   # En extract_latam_works(), cambiar:
   offset = 0  # ‚Üê Cambiar a 50000, 100000, etc.
   ```

## Alternativa: Cargar M√°s Datos

Si es posible, considera cargar estas tablas adicionales del snapshot:

### **Prioridad Alta**
1. `institutions` - Para detecci√≥n precisa de pa√≠ses
2. `sources` - Para identificar journals
3. `works_primary_location` - Para relacionar works con sources

### **Prioridad Media**
4. `works_open_access` - Para m√©tricas de OA
5. `works_concepts` - Para an√°lisis tem√°tico

Con estas tablas, podr√≠as usar el script completo `data_collector_postgres.py` que es mucho m√°s preciso.

## Comparaci√≥n de Scripts

| Script | Requiere | Precisi√≥n | Velocidad |
|--------|----------|-----------|-----------|
| `data_collector_postgres.py` | Todas las tablas | ‚úÖ Alta (95%+) | ‚ö° R√°pida (con √≠ndices) |
| `data_collector_postgres_simple.py` | Solo works + authorships | ‚ö†Ô∏è Media (50-80%) | üêå Lenta (scan completo) |

## Pr√≥ximos Pasos

Despu√©s de extraer los datos:

1. **Verificar resultados**:
   ```bash
   python diagnose_data.py
   ```

2. **Revisar muestra**:
   ```python
   import pandas as pd
   df = pd.read_parquet('data/latin_american_works.parquet')
   print(df[['display_name', 'country_code', 'publication_year']].head(20))
   ```

3. **Calcular m√©tricas** (si tienes suficientes datos):
   ```bash
   python precompute_metrics_parallel_optimized.py
   ```

## Recomendaci√≥n

Si tienes espacio en disco y ancho de banda, **carga las tablas faltantes** del snapshot de OpenAlex:

- `institutions` (~200 MB comprimido)
- `sources` (~50 MB comprimido)  
- `works_primary_location` (~500 MB comprimido)

Esto te permitir√° usar el script completo con mucha mejor precisi√≥n.
