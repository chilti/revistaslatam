# Correcciones al Script de Descarga de Datos

## Problemas Identificados y Solucionados

### üêõ **Problema 1: Journals se descargaban en cada ejecuci√≥n**

**S√≠ntoma**: Cada vez que ejecutabas el script, descargaba todos los journals de nuevo, aunque ya existieran.

**Causa**: El script no verificaba si `latin_american_journals.parquet` ya exist√≠a.

**Soluci√≥n**: 
- Agregado par√°metro `update_journals=False` (por defecto)
- El script ahora **carga** los journals existentes en lugar de descargarlos de nuevo
- Solo descarga journals si:
  - El archivo no existe
  - O se pasa `update_journals=True` expl√≠citamente

```python
# Antes (SIEMPRE descargaba):
for country in LATAM_COUNTRIES:
    country_journals = fetch_journals_by_country(country)
    all_journals.extend(country_journals)

# Ahora (solo si es necesario):
if update_journals or not os.path.exists(PARQUET_FILE):
    # Descargar journals...
else:
    # Cargar journals existentes
    df = pd.read_parquet(PARQUET_FILE)
    all_journals = df.to_dict('records')
```

---

### üêõ **Problema 2: Contador de progreso incorrecto**

**S√≠ntoma**: El script mostraba progreso incorrecto, como "Processing 50/1000" cuando solo hab√≠a 50 journals pendientes.

**Causa**: Usaba `len(all_journals)` en lugar de `len(journals_to_process)`.

**L√≠neas corregidas**:
- L√≠nea 196: `if idx % batch_size == 0 or idx == len(journals_to_process):`
- L√≠nea 310: `print(f"\n--- Progress: {idx}/{len(journals_to_process)} journals processed ---\n")`

**Impacto**: Esto podr√≠a haber causado que el script pensara que hab√≠a m√°s trabajo por hacer.

---

### üêõ **Problema 3: Posibles re-descargas**

**Causa potencial**: Si el contador estaba mal, el script podr√≠a haber intentado procesar journals ya descargados.

**Soluci√≥n**: Con los contadores corregidos, el script ahora:
1. Identifica correctamente cu√°ntos journals faltan
2. Procesa solo los faltantes
3. Muestra progreso preciso

---

## C√≥mo Usar el Script Correctamente

### **Uso Normal (Recomendado)**

```bash
python src/data_collector.py
```

**Comportamiento**:
- ‚úÖ Carga journals existentes (NO descarga de nuevo)
- ‚úÖ Descarga solo works de journals faltantes
- ‚úÖ Modo resume autom√°tico

---

### **Forzar Actualizaci√≥n de Journals**

Si quieres actualizar la metadata de journals (por ejemplo, si OpenAlex agreg√≥ nuevos campos):

```python
from src.data_collector import update_data

# Actualizar journals Y works
update_data(include_works=True, resume=True, update_journals=True)
```

---

### **Solo Actualizar Journals (sin works)**

```python
from src.data_collector import update_data

# Solo actualizar journals, no descargar works
update_data(include_works=False, update_journals=True)
```

---

## Script de Diagn√≥stico

He creado `diagnose_data.py` para verificar el estado actual:

```bash
python diagnose_data.py
```

**Informaci√≥n que muestra**:
1. ‚úÖ Total de journals y distribuci√≥n por pa√≠s
2. ‚úÖ Total de works descargados
3. ‚úÖ Journals con/sin works
4. ‚úÖ Revistas faltantes por pa√≠s
5. ‚úÖ Detecci√≥n de duplicados
6. ‚úÖ Tama√±o de archivos

---

## Diagn√≥stico del Problema Actual

### **Posibles Causas de Re-descarga de Brasil**

1. **Journals duplicados**: Si el archivo de journals tiene duplicados, el script intentar√° descargar works para cada duplicado.

2. **Works duplicados**: Si hay works duplicados en el archivo, parecer√° que hay m√°s works de los que realmente hay.

3. **Contador incorrecto**: El bug del contador podr√≠a haber hecho que el script pensara que faltaban journals por procesar.

### **Pasos para Diagnosticar**

1. **Ejecuta el script de diagn√≥stico**:
   ```bash
   python diagnose_data.py
   ```

2. **Verifica**:
   - ¬øCu√°ntos journals hay en total?
   - ¬øCu√°ntos journals tienen works descargados?
   - ¬øHay duplicados en works?
   - ¬øQu√© journals faltan por pa√≠s?

3. **Comparte el output** para que pueda ver exactamente qu√© est√° pasando.

---

## Soluci√≥n de Problemas

### **Si hay duplicados en works**

```python
import pandas as pd

# Leer works
works_df = pd.read_parquet('data/latin_american_works.parquet')

# Eliminar duplicados por 'id'
works_df_clean = works_df.drop_duplicates(subset=['id'], keep='first')

# Guardar limpio
works_df_clean.to_parquet('data/latin_american_works.parquet', index=False)

print(f"Eliminados {len(works_df) - len(works_df_clean)} duplicados")
```

### **Si hay journals duplicados**

```python
import pandas as pd

# Leer journals
journals_df = pd.read_parquet('data/latin_american_journals.parquet')

# Eliminar duplicados por 'id'
journals_df_clean = journals_df.drop_duplicates(subset=['id'], keep='first')

# Guardar limpio
journals_df_clean.to_parquet('data/latin_american_journals.parquet', index=False)

print(f"Eliminados {len(journals_df) - len(journals_df_clean)} duplicados")
```

### **Si quieres empezar de cero solo con works**

```bash
# Respaldar
mv data/latin_american_works.parquet data/latin_american_works.parquet.backup

# Ejecutar script (mantendr√° journals existentes)
python src/data_collector.py
```

---

## Mejoras Implementadas

### **1. Eficiencia**
- ‚úÖ No descarga journals innecesariamente
- ‚úÖ Ahorra tiempo y llamadas a la API
- ‚úÖ Reduce riesgo de exceder l√≠mites de API

### **2. Precisi√≥n**
- ‚úÖ Contadores de progreso correctos
- ‚úÖ Mensajes claros sobre qu√© se est√° haciendo
- ‚úÖ Mejor logging

### **3. Control**
- ‚úÖ Par√°metro `update_journals` para control expl√≠cito
- ‚úÖ Modo resume funciona correctamente
- ‚úÖ F√°cil de diagnosticar problemas

---

## Ejemplo de Salida Esperada

### **Primera Ejecuci√≥n (sin datos)**
```
Starting data update from OpenAlex...

============================================================
DOWNLOADING JOURNAL METADATA
============================================================
Fetching journals for MX...
Found 150 journals for MX.
Fetching journals for BR...
Found 450 journals for BR.
...
Saving 1200 journal records to data/latin_american_journals.parquet...
Journal data update complete.

============================================================
Starting Works (articles) download...
This will download articles for 1200 journals.
Will process 1200 journals.
============================================================
```

### **Ejecuci√≥n Posterior (con journals existentes)**
```
Starting data update from OpenAlex...

============================================================
LOADING EXISTING JOURNAL METADATA
============================================================
Using existing journal file: data/latin_american_journals.parquet
Loaded 1200 journals from cache.

============================================================
Starting Works (articles) download...
This will download articles for 1200 journals.
Found 800 journals already downloaded.
RESUME MODE: Skipping 800 already downloaded journals.
Will process 400 journals.
============================================================
```

---

## Recomendaciones

1. **Ejecuta el diagn√≥stico primero**:
   ```bash
   python diagnose_data.py
   ```

2. **Revisa el output** para entender el estado actual

3. **Si hay duplicados**, l√≠mpialos con los scripts de arriba

4. **Contin√∫a la descarga** con el script corregido:
   ```bash
   python src/data_collector.py
   ```

5. **Monitorea el progreso** - ahora deber√≠a ser preciso

---

## Contacto

Si despu√©s de ejecutar el diagn√≥stico sigues viendo comportamiento extra√±o, comparte:
- Output de `diagnose_data.py`
- √öltimas l√≠neas del log cuando ejecutas `data_collector.py`
- Cu√°ntos journals/works esperas vs cu√°ntos tienes

Esto me ayudar√° a identificar si hay alg√∫n otro problema.
